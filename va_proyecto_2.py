# -*- coding: utf-8 -*-
"""va_proyecto_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/fespinozav/2c97008d74dd46a3160b1703097910cb/va_proyecto_2.ipynb

# **Actividad: Desarrollo y Mejora de Proyectos de Visión Computacional con Técnicas Híbridas**

**Objetivo:** Desarrollar un sistema funcional que integre técnicas tradicionales de procesamiento digital de imágenes con enfoques de deep learning para abordar un problema específico de visión por computador, utilizando como base códigos de referencia disponibles en repositorios públicos (por ejemplo, GitHub). Los estudiantes deberán adaptar, optimizar y complementar dichos códigos, documentando su trabajo con fundamentos técnicos y justificaciones metodológicas.


### Proyecto: **Estimación de profundidad a partir de imágenes monoculares con técnicas híbridas de visión por computadora**

#### **Autores**:


1.   Felipe Ernesto Espinoza Vidal
2.   Lizandra Hernández Peña

# **1. Investigación y análisis del problema**

La estimación de profundidad monocular (MDE) es un desafío clave para sistemas autónomos, realidad aumentada y robótica. Obtener un mapa de profundidad preciso sin información estéreo o sensores externos permite aplicaciones más accesibles y económicas.

Este tipo de técnica también llamada percepción de profundidad a partir de una sola imagen, busca predecir, a partir de una única imagen RGB, el valor de profundidad por píxel (un mapa de profundidades). Es clave para comprender la geometría de la escena en 3D y habilita aplicaciones como conducción autónoma, robótica y AR, sin requerir hardware especializado como LiDAR o cámaras estéreo. Su dificultad proviene de que debe inferir relaciones complejas (perspectiva, sombreado, texturas) a partir de proyecciones 2D. Factores como iluminación, oclusiones y regiones con poca textura degradan el desempeño.

### Aplicaciones:
- Realidad aumentada en dispositivos móviles
- Navegación autónoma
- Reconstrucción 3D
- Robots de bajo costo
- Asistentes de movilidad

### Contexto con otras técnicas de profundidad

Frente a enfoques activos (p. ej., cámaras estéreo con proyección de patrones IR o LiDAR), la MDE se apoya en aprendizaje automático para estimar (no medir) profundidad desde una sola vista. Esto reduce dependencia de sensores específicos y abre nuevos casos de uso, aunque mantiene ambigüedades de escala y sensibilidad a condiciones de captura.

### Enfoques de aprendizaje
- Supervisado (CNN/encoder-decoder): redes convolucionales aprenden la relación implícita entre color y profundidad; es común el transfer learning con backbones tipo ResNet o variantes U-Net.
- Transformers (ViT): arquitecturas con auto-atención han mostrado mejoras >28% respecto a CNN en MDE, manteniendo el esquema encoder-decoder.
- No supervisado / auto-supervisado: entrenan con pares estéreo o secuencias de video, reconstruyendo una vista desde la otra y usando consistencia izquierda-derecha o técnicas como auto-masking y multiescala a resolución completa para mejorar la calidad. Reducen la dependencia de ground truth denso.

### Pipeline de referencia

Un flujo reproducible con TensorFlow/Keras y el conjunto DIODE (validación, ~2.6 GB)[1] :
1.	Preparación de datos (RGB + mapas de profundidad + máscaras),
2.	Preprocesamiento (recorte, log-depth, máscara),
3.	Modelo encoder-decoder con ResNet50 y 5 capas de decoder,
4.	Pérdida combinada (MSE + Huber) y métrica MAE,
5.	Entrenamiento (ej. 60 épocas, batch 32) e inferencia.

### Modelos preentrenados (SOTA práctico)

Para rendimiento y tiempos mejores, se recomienda inferir con Depth Anything V2 (variante “Small”), que reporta resultados de estado del arte en benchmarks como KITTI, y puede usarse con unas pocas líneas usando transformers pipeline.  

### Retos del problema

La MDE es inherentemente ambigua (múltiples escenas 3D pueden proyectar la misma imagen 2D). Persisten desafíos como oclusiones, regiones sin textura y la ambigüedad de escala; resolverlos requiere señales contextuales robustas y/o entrenamiento con señales de reconstrucción geométrica.

### Aplicaciones y proyección

Las aplicaciones abarcan conducción autónoma, robótica, AR y modelado 3D. La tendencia va hacia integrar MDE con tareas como detección, segmentación y entendimiento de escena para sistemas más completos, con Transformers como línea prometedora de investigación.

### Análisis del problema y primeras investigaciones
1.	Formulación: dado un input $I\in\mathbb{R}^{H\times W\times 3}$, predecir $D\in\mathbb{R}^{H\times W}$. El objetivo es minimizar el error entre D y profundidad de referencia (si existe), o pérdidas fotométricas/geométricas en esquemas auto-supervisados.
2.	Estrategia base: prototipar con el pipeline Keras + DIODE para establecer un baseline reproducible, y contrastar con Depth Anything V2 para medir la brecha entre un modelo simple y uno SOTA.
3.	Riesgos técnicos: cuidar iluminación, texturas pobres y escala; considerar datos variados y/o pérdidas geométricas para mitigar.


---
[1] “Monocular Depth Estimation: Technical Exploration”, Gaudenz Boesch, Viso.ai, 30 de octubre de 2024. (https://viso.ai/computer-vision/monocular-depth-estimation/)

# 2. Revisión de soluciones existentes

##**Repositorio 1: MiDaS - Intel ISL**

Este repositorio ofrece modelos preentrenados como DPT_Large, DPT_Hybrid y versiones de MiDaS entrenadas en múltiples datasets (KITTI, NYUv2, etc.).

**Aspectos positivos**:
- Precisión SOTA
- Buen soporte y documentación
- Generaliza bien a escenas naturales

**Aspectos negativos**:
- No produce profundidad absoluta (métrica), solo relativa
- Requiere buena GPU para inferencia rápida
- Entrenado con datasets limitados

##**Repositorio 2: PathFusion:**
PatchFusion es un framework end-to-end basado en tiles para estimación métrica de profundidad monocular en imágenes de alta resolución. El sistema utiliza una estrategia de división en parches que procesa imágenes de alta resolución dividiéndolas en patches más pequeños, procesándolos individualmente y fusionando los resultados [2].

**Modelos Base Soportados:**

*   ZoeDepth-N (Zhyever/patchfusion_zoedepth)
*   Depth-Anything-vits (Zhyever/patchfusion_depth_anything_vits14)
*   Depth-Anything-vitb (Zhyever/patchfusion_depth_anything_vitb14)
*   Depth-Anything-vitl (Zhyever/patchfusion_depth_anything_vitl14)

**Aspectos Positivos**
*   Alta Precisión Local: Al centrarse en parches, logra un refinamiento más fino en áreas complejas.

*   Robustez ante Ruido: Puede mejorar mapas de profundidad ruidosos o de baja calidad inicial.


**Aspectos negativos**
*   Restricciones específicas: las dimensiones de imagen deben ser divisibles por 2*patch_split_num
*   Requiere Múltiples Vistas: Su desempeño óptimo se logra con múltiples imágenes de la escena.
*   Alto Costo Computacional: El procesamiento por parches y la integración de redes profundas puede ser costoso en tiempo y hardware.

---
[2] "PatchFusion: An End-to-End Tile-Based Framework
 for High-Resolution Monocular Metric Depth Estimation,  Zhenyu Li, Shariq Farooq Bhat, Peter Wonka, 4 de diciembre de 2023, https://arxiv.org/pdf/2312.02284

## **Repositorio 3: Marigold:**
*   Reutilización de modelos de imágenes generativas para el análisis de visión por computadora
*   Predice las normales de la superficie a partir de imágenes RGB, estimando la orientación 3D de las superficies en cada píxel


Marigold es compatible con tres tipos principales de modelos especializados en diferentes tareas de visión por computadora:


*   MarigoldDepthPipeline: Para estimación monocular de profundidad
*   MarigoldNormalsPipeline: Para predicción de normales de superficie
*   MarigoldIIDPipeline: Para descomposición en múltiples componentes

Las ventajas de Marigold se centran en su capacidad de reutilizar conocimiento preentrenado y lograr excelente generalización con entrenamiento eficiente. Las desventajas principales están relacionadas con requisitos computacionales y complejidad operacional. El modelo representa un avance significativo en la adaptación de modelos generativos para tareas de análisis de imágenes, pero requiere recursos considerables para su implementación práctica.

##**Repositorio 4: ZoeDepth - Intel ISL**

El objetivo de este modelo es predecir profundidad métrica (en unidades reales) a partir de una sola imagen y generalizar bien fuera del dominio, combinando lo mejor de dos mundos: la robustez de los modelos de profundidad relativa (tipo MiDaS) y la escala métrica de los modelos supervisados por sensores.[3]

ZoeDepth usa un backbone estilo DPT/MiDaS y añade una cabeza de “bins métricos” (metric bins module) que aprende discretizaciones adaptativas de profundidad para producir valores en escala real. Se pre-entrena con profundidad relativa en 12 datasets y luego se ajusta con profundidad métrica en NYU Depth v2 (indoor) y KITTI (outdoor). En su variante multi-dominio (ZoeD-M12-NK), incorpora dos cabezas métricas (N para indoor, K para outdoor) y un clasificador latente que enruta automáticamente cada imagen a la cabeza adecuada en inferencia. Resultado: cero-shot sólido en múltiples dominios sin perder escala.

El repositorio en github [4] publica tres configuraciones: ZoeD_N (NYU), ZoeD_K (KITTI) y ZoeD_NK (mixta). Se cargan vía Torch Hub y devuelven mapas de profundidad en 16-bit/NumPy/Tensor. La release inicial reporta métricas (REL) en varios benchmarks y provee scripts de evaluación/entrenamiento. Nota actual: el repositorio quedó archivado (read-only) el 5 de mayo de 2025.

**Modelos/variantes oficiales**

- 	ZoeD-M12-N, ZoeD-M12-K, ZoeD-M12-NK (indoor, outdoor y mixto con ruteo de dominio). Backbone: DPT con BEiT-Large 384 (BEiT-L-384). Disponibles en Github.

**Aspectos positivos**

- Profundidad métrica real (no solo relativa), con buena generalización zero-shot entre indoor/outdoor (variante NK).
- Uso sencillo vía Torch Hub y salida en 16-bit / NumPy / Tensor lista para pipelines.

**Aspectos negativos**

- Repo archivado por Intel en may del 2025: sin mantenimiento oficial (parches/bugs/nuevas releases).
- Backbone único oficial (BEiT-L-384) menos encoders que MiDaS. Que se traduce en menos calidad y latencia según hardware.

**¿MidaS o ZoeDepth?**

En general si el objetivo es una métrica real con buena generalización, ZoeDepth es la opción. Si se prioriza robustez universal y rapidez sin requerir escala absoluta, MiDaS sigue siendo un caballo de batalla muy sólido; incluso la propia guía de MiDaS sugiere ZoeDepth cuando necesitas métrica, además posee una versión directa para PyTorch, lo que lo hace muy rápido y facil de testear.



---
[3] Bhat, S. F., Birkl, R., Wofk, D., Wonka, P., & Müller, M. (2023). Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288
[4] https://github.com/isl-org/ZoeDepth

##**Repositorio 5: Depth-Anything-V2**

El objetivo de Depth Anything V2 (DA-V2) es servir como modelo fundacional de estimación de profundidad monocular con alta generalización zero-shot y opción de profundidad métrica mediante fine-tuning. Frente a modelos basados en Stable Diffusion, DA-V2 reporta mayor precisión, menos parámetros y >10x más velocidad de inferencia. La clave es un esquema teacher-student que sustituye el ground truth real etiquetado por 595 K imágenes sintéticas y aprovecha 62 M+ de imágenes reales no etiquetadas para pseudo-rotular y transferir capacidad al student. Posteriormente, se ajusta a profundidad métrica con etiquetas métricas específicas.

DA-V2 mantiene la arquitectura tipo DPT con ViT como encoder y un decoder ligero; en V2 se corrige la selección de características intermedias de DINOv2 para el decoding. El pipeline oficial incluye scripts para imágenes y video, Gradio demo, y soporte directo en Hugging Face Transformers (también hay puertos a Core ML, TensorRT, ONNX y Transformers.js). Por defecto, usa input size 518, y expone encoders vits/vitb/vitl/vitg. Para métrica, el repositorio provee una ruta de fine-tuning y modelos “metric-depth” pequeños y base.

DA-V2 mantiene la arquitectura tipo DPT con ViT como encoder y un decoder ligero; en V2 se corrige la selección de características intermedias de DINOv2 para el decoding. El pipeline oficial incluye scripts para imágenes y video, Gradio demo, y soporte directo en Hugging Face Transformers (también hay puertos a Core ML, TensorRT, ONNX y Transformers.js). Por defecto, usa input size 518, y expone encoders vits/vitb/vitl/vitg. Para métrica, el repositorio provee una ruta de fine-tuning y modelos “metric-depth” pequeños y base.

El repositorio en GitHub publica pesos preentrenados para profundidad relativa (Small/Base/Large; Giant “coming soon”) y detalla un snippet de uso tanto en PyTorch como vía pipeline("depth-estimation") en Transformers. Incluye además notas de licencia: Small = Apache-2.0, Base/Large/Giant = CC-BY-NC-4.0. Actualizaciones recientes muestran mantenimiento activo (p. ej., “Video Depth Anything”, “Prompt Depth Anything”, soporte en Transformers/Core ML).

**Modelos/variantes oficiales**
- Depth-Anything-V2-Small (≈ 24.8 M params), Base (≈ 97.5 M), Large (≈ 335.3 M), Giant (≈ 1.3 B, en preparación). Encoders: vits, vitb, vitl, vitg. Relative depth por defecto; metric depth disponible vía fine-tuning (modelos “small/base métricos” liberados).

**Aspectos positivos**

- Rendimiento-velocidad: más rápido y preciso que alternativas basadas en SD, con inferencias >10x más rápidas reportadas por los autores.
- Generalización robusta (zero-shot) gracias a la escala de datos (595 K sintéticas + 62 M+ reales no etiquetadas) y al esquema teacher-student.
- Ecosistema amplio: soporte oficial en Transformers, Core ML, TensorRT, ONNX y web (Transformers.js); fácil de integrar en pipelines Python y móviles.
- Opción métrica: fine-tuning a profundidad métrica con pesos publicados (Small/Base), útil para aplicaciones que requieren escala real.

**Aspectos negativos**

- Licencia mixta: solo Small es Apache-2.0; Base/Large/Giant son CC-BY-NC-4.0 (no comercial), lo que puede limitar usos empresariales.
- Giant no estable: el modelo Giant figura “coming soon”; según el README, aún no hay release consolidada.
- Métrica requiere ajuste: aunque hay pesos métricos, la mejor precisión métrica suele depender del fine-tuning por dominio/dataset y del preprocesamiento correcto (alineado a la guía del repo).

**Referencias**
- https://github.com/isl-org/ZoeDepth
- Yang, L., Kang, B., Huang, Z., Zhao, Z., Xu, X., Feng, J., & Zhao, H. (2024). Depth anything v2. Advances in Neural Information Processing Systems, 37, 21875-21911.

## **En resumen**

- MiDaS: sin escala absoluta → requiere calibración o un módulo métrico adicional.
- ZoeDepth: buen métrico zero-shot, pero sin mantenimiento oficial.
- Depth Anything V2: métrico disponible por fine-tuning y modelos “métricos” pequeños/base; para dominios específicos puede requerir ajuste adicional y respetar licencias.

# 3. Técnicas de Procesamiento digital de imágenes (PDI)

## Experimento 1
Se experimentó con un  pre/post-procesamiento ligero y justificable:
1.	Normalización fotométrica (balance de blancos, corrección gamma) para reducir variaciones de iluminación.

2.	Redimensionado y letterbox a la resolución de inferencia (p.ej., 518 px en DA-V2) para preservar aspect ratio y evitar artefactos.
3.	Realce de bordes (Laplaciano) y filtro bilateral/guided filter como post-procesado del mapa de profundidad para suavizado con preservación de contornos (mejora visual y de bordes).
4.	Máscaras morfológicas de cielo/suelo (cuando existan priors), útiles para estabilizar la escala o eliminar outliers en regiones problemáticas (cielo casi infinito).
5.	Pirámides multi-escala para refinar detalles finos tras la predicción base. (opcional)
"""

# Descargando imágenes para procesar
!gdown 1m6oHAzuhQAyxYLZcb4zku7QVLy_qNIx9
!gdown 1iPjs0G7yJuwz8Upe24CujV8It3QMWiHC

import cv2
import numpy as np

# Leer imagen
img = cv2.imread("WhatsApp Image 2024-05-14 at 10.35.41.jpeg")

from google.colab.patches import cv2_imshow
cv2_imshow(img)

# Cambiando a dos canales
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Filtro Gaussiano para reducción de ruido
blurred = cv2.GaussianBlur(gray, (5, 5), 0)

# Ecualización de histograma
equalized = cv2.equalizeHist(blurred)

# Detección de bordes (opcional, solo visual)
edges = cv2.Canny(equalized, 100, 200)

cv2.imwrite("preprocessed.jpg", equalized)
cv2.imwrite("edges.jpg", edges)

# imagen original
cv2_imshow(img)

# imagen ecualizada
cv2_imshow(equalized)

# bordes de la imagen
cv2_imshow(edges)

"""# 4. Complementación con deep learning

## 4.1 Experimentación con modelos de de deep learning

Usando **MiDaS**.
- https://deepwiki.com/isl-org/MiDaS
- https://github.com/isl-org/MiDaS
- https://pytorch.org/hub/intelisl_midas_v2/
"""

!pip install torch torchvision opencv-python matplotlib --quiet

import cv2
import torch
import numpy as np
import matplotlib.pyplot as plt

# Configurar dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Cargar modelo y transformaciones
model_type = "DPT_Large"  # Puedes cambiar a "DPT_Hybrid" si quieres algo más liviano
midas = torch.hub.load("intel-isl/MiDaS", model_type)
midas.to(device).eval()

midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
transform = midas_transforms.dpt_transform

# Cargar imagen y preprocesamiento clásico
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Opción: filtrar y ecualizar
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
blurred = cv2.GaussianBlur(gray, (5, 5), 0)
equalized = cv2.equalizeHist(blurred)

# Mostrar ecualizada
plt.figure(figsize=(10,3))
plt.subplot(1,2,1)
plt.imshow(gray, cmap='gray')
plt.title("Gris original")
plt.axis('off')
plt.subplot(1,2,2)
plt.imshow(equalized, cmap='gray')
plt.title("Ecualización de histograma")
plt.axis('off')
plt.show()

# Transformación para MiDaS
input_batch = transform(img_rgb).to(device)

# Inferencia
with torch.no_grad():
    prediction = midas(input_batch)
    prediction = torch.nn.functional.interpolate(
        prediction.unsqueeze(1),
        size=img.shape[:2],
        mode="bicubic",
        align_corners=False,
    ).squeeze()

# Normalizar profundidad
depth_map = prediction.cpu().numpy()
depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())

# Mostrar mapa de profundidad
plt.figure(figsize=(8,4))
plt.imshow(depth_map, cmap='inferno')
plt.title("Mapa de profundidad estimado")
plt.axis("off")
plt.show()

"""## 4.2 Experimentación con modelos de deep learning

Usando **PathFusion**
*   https://deepwiki.com/zhyever/PatchFusion
*  https://github.com/zhyever/PatchFusion
"""

# Commented out IPython magic to ensure Python compatibility.
#  Configuración inicial
!git clone https://github.com/zhyever/PatchFusion.git
# %cd PatchFusion

# Instalar NumPy
!pip uninstall numpy -y
# Usar NumPy 1.26.0
!pip install numpy==1.26.0

# Verificar NumPy
import numpy as np
print(f"NumPy version: {np.__version__}")

# Desinstalar e instalar PyTorch con versiones específicas
!pip uninstall torch torchvision torchaudio -y
!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118

# Verificar PyTorch
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# Instalar dependencias
!pip install opencv-python==4.8.1.78
!pip install huggingface-hub==0.20.1
!pip install timm==0.9.2
!pip install mmengine==0.10.2
!pip install kornia==0.7.2
!pip install einops
!pip install matplotlib
!pip install transformers torchmetrics --upgrade

# Configurar variables de entorno
import os
import sys

colab_path = "/content/PatchFusion"
sys.path.append(colab_path)
sys.path.append(f"{colab_path}/external")
os.environ['PYTHONPATH'] = f"{colab_path}:{colab_path}/external"

# PASO 4: Subir imágenes
!mkdir -p examples

from google.colab import files
print("Selecciona las imágenes que quieres procesar:")
uploaded = files.upload()

import shutil
for filename in uploaded.keys():
    shutil.move(filename, f'examples/{filename}')
    print(f"Imagen {filename} subida correctamente")

# Ejecutar inferencia
# Importar librerías una por una
try:
    import cv2
    print("✓ OpenCV importado correctamente")
except Exception as e:
    print(f"✗ Error con OpenCV: {e}")

try:
    import torch
    import torch.nn.functional as F
    print("✓ PyTorch importado correctamente")
except Exception as e:
    print(f"✗ Error con PyTorch: {e}")

try:
    from torchvision import transforms
    print("✓ Torchvision importado correctamente")
except Exception as e:
    print(f"✗ Error con Torchvision: {e}")

try:
    from estimator.models.patchfusion import PatchFusion
    print("✓ PatchFusion importado correctamente")
except Exception as e:
    print(f"✗ Error con PatchFusion: {e}")

# Configurar dispositivo
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Usando dispositivo: {DEVICE}")

# Cargar modelo
model_name = 'Zhyever/patchfusion_depth_anything_vitl14'
print("Cargando modelo desde HuggingFace...")
model = PatchFusion.from_pretrained(model_name).to(DEVICE).eval()
print("Modelo cargado exitosamente")

# Configuración optimizada
image_raw_shape = [1080, 1920]
image_resizer = model.resizer

tile_cfg = dict()
tile_cfg['image_raw_shape'] = image_raw_shape
tile_cfg['patch_split_num'] = [2, 2]

# PROCESAMIENTO BÁSICO
import cv2
from scipy import ndimage
import matplotlib.pyplot as plt

for filename in uploaded.keys():
    print(f"\nProcesando imagen: {filename}")

    # Cargar imagen original
    image_path = f'./examples/{filename}'
    image_bgr = cv2.imread(image_path)

    if image_bgr is None:
        print(f"Error: No se pudo cargar la imagen {filename}")
        continue

    # Convertir a RGB para matplotlib
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)

    # 1. Escala de grises
    image_gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    # 2. Filtro gaussiano (suavizado)
    # Aplicar filtro gaussiano con kernel 15x15 y sigma=2
    image_gaussian = cv2.GaussianBlur(image_rgb, (15, 15), 2)

    # 3. Detección de bordes usando Canny
    # Primero convertir a escala de grises para Canny
    edges = cv2.Canny(image_gray, threshold1=50, threshold2=150)

    # Visualizar los tres procesamientos
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # Imagen original
    axes[0, 0].imshow(image_rgb)
    axes[0, 0].set_title(f'Original: {filename}')
    axes[0, 0].axis('off')

    # Escala de grises
    axes[0, 1].imshow(image_gray, cmap='gray')
    axes[0, 1].set_title('Escala de Grises')
    axes[0, 1].axis('off')

    # Filtro gaussiano
    axes[1, 0].imshow(image_gaussian)
    axes[1, 0].set_title('Filtro Gaussiano (Suavizado)')
    axes[1, 0].axis('off')

    # Detección de bordes
    axes[1, 1].imshow(edges, cmap='gray')
    axes[1, 1].set_title('Detección de Bordes (Canny)')
    axes[1, 1].axis('off')

    plt.tight_layout()
    plt.show()

    # Guardar las versiones procesadas
    cv2.imwrite(f'gray_{filename}', image_gray)
    cv2.imwrite(f'gaussian_{filename}', cv2.cvtColor(image_gaussian, cv2.COLOR_RGB2BGR))
    cv2.imwrite(f'edges_{filename}', edges)

    print(f"Versiones procesadas guardadas para {filename}")

# Procesar imágenes
import matplotlib.pyplot as plt

for filename in uploaded.keys():
    print(f"\nProcesando imagen: {filename}")

    image_path = f'./examples/{filename}'
    image = cv2.imread(image_path)

    if image is None:
        print(f"Error: No se pudo cargar la imagen {filename}")
        continue

    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0
    image = transforms.ToTensor()(np.asarray(image)) #convierte la imagen de NumPy array a tensor de PyTorch

    image_lr = image_resizer(image.unsqueeze(dim=0)).float().to(DEVICE) #redimensiona la imagen a la resolución de procesamiento del modelo base
    image_hr = F.interpolate(image.unsqueeze(dim=0), image_raw_shape, mode='bicubic', align_corners=True).float().to(DEVICE)

    print("Ejecutando inferencia...")
    depth_prediction, _ = model(
        mode='infer',
        cai_mode='r32',
        process_num=2,
        image_lr=image_lr,
        image_hr=image_hr,
        tile_cfg=tile_cfg
    )
    depth_prediction = F.interpolate(depth_prediction, image.shape[-2:])[0, 0].detach().cpu().numpy()
   # Visualizar resultados
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    ax1.imshow(image.permute(1, 2, 0))
    ax1.set_title(f'Imagen Original: {filename}')
    ax1.axis('off')

    depth_colored = plt.cm.viridis(depth_prediction / depth_prediction.max())
    ax2.imshow(depth_colored)
    ax2.set_title('Mapa de Profundidad')
    ax2.axis('off')

    plt.tight_layout()
    plt.show()

    result_filename = f'depth_{filename.split(".")[0]}.png'
    plt.imsave(result_filename, depth_colored)
    print(f"Resultado guardado como: {result_filename}")

print("\n¡Procesamiento completado!")

"""## 4.3 Experimentación con modelos de deep learning

Usando **Marigold**
*  https://deepwiki.com/prs-eth/Marigold
*  https://github.com/prs-eth/Marigold
"""

# Commented out IPython magic to ensure Python compatibility.
 # Clonar el repositorio
!git clone https://github.com/prs-eth/Marigold.git
# %cd Marigold

# Instalar dependencias
!pip install -r requirements.txt

from google.colab import files
import os
import shutil
import matplotlib.pyplot as plt
from PIL import Image
from glob import glob

# 1. Subir imágenes
os.makedirs('input/mis_imagenes', exist_ok=True)

print("Selecciona las imágenes desde tu ordenador:")
uploaded = files.upload()

# Mover archivos
for filename in uploaded.keys():
    shutil.move(filename, f'input/mis_imagenes/{filename}')

print(f" Se subieron {len(uploaded)} imágenes")

# 2. MOSTRAR IMÁGENES ORIGINALES
def mostrar_imagenes_originales(input_dir, max_images=5):
    extensions = [".jpg", ".jpeg", ".png"]
    image_files = []

    for ext in extensions:
        image_files.extend(glob(os.path.join(input_dir, f"*{ext}")))
        image_files.extend(glob(os.path.join(input_dir, f"*{ext.upper()}")))

    image_files = sorted(image_files)

    print(f" IMÁGENES ORIGINALES ({len(image_files)} encontradas):")
    print("=" * 60)

    for i, img_path in enumerate(image_files[:max_images]):
        img = Image.open(img_path)
        filename = os.path.basename(img_path)

        plt.figure(figsize=(12, 8))
        plt.imshow(img)
        plt.title(f'ORIGINAL: {filename} | {img.size[0]}x{img.size[1]} | {img.mode}',
                 fontsize=14, fontweight='bold')
        plt.axis('off')
        plt.show()

# Mostrar imágenes originales
mostrar_imagenes_originales('input/mis_imagenes')

# 3. Ejecutar procesamiento
print("\n PROCESANDO IMÁGENES...")
!python script/depth/run.py \
    --checkpoint prs-eth/marigold-depth-v1-1 \
    --input_rgb_dir input/mis_imagenes \
    --output_dir output/mis_resultados \
    --fp16

# 4. Mostrar comparación
def comparar_original_vs_procesada(input_dir, output_dir, max_images=3):
    # Buscar imágenes originales
    extensions = [".jpg", ".jpeg", ".png"]
    original_files = []

    for ext in extensions:
        original_files.extend(glob(os.path.join(input_dir, f"*{ext}")))
        original_files.extend(glob(os.path.join(input_dir, f"*{ext.upper()}")))

    original_files = sorted(original_files)

    print(f"\n COMPARACIÓN ORIGINAL vs PROCESADA:")
    print("=" * 60)

    for i, orig_path in enumerate(original_files[:max_images]):
        orig_img = Image.open(orig_path)
        orig_filename = os.path.basename(orig_path)

        # Buscar imagen procesada correspondiente
        base_name = os.path.splitext(orig_filename)[0]
        depth_colored_path = os.path.join(output_dir, "depth_colored", f"{base_name}_depth_colored.png")

        if os.path.exists(depth_colored_path):
            depth_img = Image.open(depth_colored_path)

            # Mostrar lado a lado
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))

            ax1.imshow(orig_img)
            ax1.set_title(f'ORIGINAL\n{orig_filename}', fontsize=12, fontweight='bold')
            ax1.axis('off')

            ax2.imshow(depth_img)
            ax2.set_title(f'MAPA DE PROFUNDIDAD\n{base_name}_depth_colored.png', fontsize=12, fontweight='bold')
            ax2.axis('off')

            plt.tight_layout()
            plt.show()
        else:
            print(f" No se encontró resultado para {orig_filename}")

# Comparar resultados
comparar_original_vs_procesada('input/mis_imagenes', 'output/mis_resultados')

"""## 5. Adaptación de código

El proyecto alojado en el repositorio de github [Depth-estimation](https://github.com/fespinozav/Depth-estimation) implementa un sistema de estimación de profundidad monocular en tiempo real y por lotes de imágenes, basado en MiDaS (Intel-ISL). Está diseñado como una aplicación parametrizable por CLI y adaptable a distintos entornos de captura y hardware.

**Entradas**: Webcam, archivo de video o carpeta de imágenes.
**Dispositivo**: Selección automática o forzada entre CUDA/MPS/CPU con opción de FP16 en CUDA para acelerar la inferencia.
**Modelos disponibles**: MiDaS DPT_Large, MiDas DPT_Hybrid o MiDaS_small, aplicando la transformación adecuada a cada variante.

**Pipeline**:
1.  Lectura y encuadre al tamaño objetivo
2.  Inferencia con MiDaS
3.  Re-escalado del mapa a la resolución original
4.  normalización por percentiles (p2-p98) para reducir parpadeo
5.  Post-procesado con técnicas de procesamiento digital de imágenes opcional con filtro bilateral que suaviza y preserva
7.  Filtro digital de paso bajo en tiempo, el cual suaviza mapas de profundidad y reduce el efecto "flicker" de parpadeo mediante un modelo matemático temporal por pixel cuando la fuente es como en este caso un video o webcam.

**Visualización**: muestra lado a lado (RGB | profundidad con colormap MAGMA) y overlay de FPS.

![Visualización](https://github.com/fespinozav/Depth-estimation/blob/main/informe/img1.png?raw=true)

**Salidas/outputs**:

- profundidad cruda (.npy float32),
- PNG 16-bit normalizado (apto para análisis cuantitativo),
- PNG coloreado para informes,
- y video MP4 con la vista combinada (en modo video).

**Trazabilidad**: registra en log.csv parámetros de ejecución (modelo, dispositivo, tamaño, flags) y FPS por cuadro.

**Usabilidad**: atajos q/x/ESC para salir y manejo del cierre por botón “X” de la ventana; soporta backends de captura avfoundation/dshow/v4l2.

---

**Referencias:**

- https://pytorch.org/hub/intelisl_midas_v2/
- https://visionbook.mit.edu/temporal_filters_v2.html
- https://visionbook.mit.edu/3d_learning.html
- Hoffman DM, Karasev VI, Banks MS. Temporal presentation protocols in stereoscopic displays: Flicker visibility, perceived motion, and perceived depth. J Soc Inf Disp. 2011 Mar 1;19(3):271-297. doi: 10.1889/JSID19.3.271. PMID: 21572544; PMCID: PMC3092720.

### 5.1 **Pseudo código**
"""

# -----------------------------
# 3.b Fuentes de entrada y procesamiento de imágenes
# -----------------------------
def process_webcam(args, midas, transform, device):
    # Backend opcional (macOS: avfoundation suele ser más estable)
    backend = backend_flag(args.backend)
    cap = cv.VideoCapture(args.cam_index if args.cam_index is not None else 0,
                          backend if backend is not None else 0)
    cap.set(cv.CAP_PROP_FRAME_WIDTH, args.cam_width)
    cap.set(cv.CAP_PROP_FRAME_HEIGHT, args.cam_height)

    if not cap.isOpened():
        raise RuntimeError("No se pudo abrir la webcam.")

    out_dir = get_output_dir(args)
    ensure_dir(out_dir)
    csv_file, writer = write_csv_header(out_dir / "log.csv")

    ema = None
    writer_video = None
    out_path_webcam = str(out_dir / "webcam_depth_colored.mp4")
    try:
        t_prev = time.perf_counter()
        idx = 0
        while True:
            ok, frame = cap.read()
            if not ok:
                print("Frame perdido; intentando continuar…")
                continue

            t0 = time.perf_counter()
            depth = run_inference(frame, midas, transform, device, args.size, args.fp16)
            # -----------------------------
            # 4.  Normalización por percentiles
            # -----------------------------
            depth_norm = percentile_normalize(depth, args.p_low, args.p_high)
            depth_u8 = (depth_norm * 255).astype(np.uint8)
            depth_u8 = apply_postprocess(depth_u8, args.bilateral)

            # -----------------------------
            # 5.  Suavizado temporal (EMA)
            # -----------------------------
            if args.ema_alpha is not None:
                if ema is None:
                    ema = depth_u8.copy()
                else:
                    a = float(args.ema_alpha)
                    ema = (a * ema + (1.0 - a) * depth_u8).astype(np.uint8)
                vis_u8 = ema
            else:
                vis_u8 = depth_u8

            depth_color = colorize(vis_u8)

            # Mostrar lado a lado
            viz = cv.hconcat([frame, depth_color])

            # Inicializa grabación MP4 en el primer frame si se solicitó
            if args.record_webcam and writer_video is None:
                fourcc = cv.VideoWriter_fourcc(*"mp4v")
                writer_video = cv.VideoWriter(out_path_webcam, fourcc, args.webcam_fps, (viz.shape[1], viz.shape[0]))
                if not writer_video.isOpened():
                    print(f"[WARN] No se pudo abrir el VideoWriter para {out_path_webcam}")
                else:
                    print(f"[INFO] Grabando webcam a {out_path_webcam} @ {args.webcam_fps} FPS")

            # Si está activo, escribir frame al MP4
            if writer_video is not None:
                writer_video.write(viz)

            # FPS
            t1 = time.perf_counter()
            fps = 1.0 / max(t1 - t_prev, 1e-6)
            t_prev = t1

            cv.putText(
                viz,
                f"{args.model} | {device.type.upper()} | {fps:5.1f} FPS",
                (10, 25),
                cv.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2,
            )

            if not args.no_display:
                cv.imshow("MiDaS Depth (webcam)", viz)
                # Cerrar si el usuario presiona el botón 'X' de la ventana
                if window_should_close("MiDaS Depth (webcam)"):
                    break

            # Guardado por frame (opcional)
            if args.save_every_n > 0 and (idx % args.save_every_n == 0):
                base = f"webcam_{idx:06d}"
                save_artifacts(depth, depth_u8, depth_color, out_dir, base, args)

            # Log CSV
            writer.writerow([
                time.strftime("%Y-%m-%d %H:%M:%S"),
                "webcam",
                idx,
                "",
                args.model,
                device.type,
                args.size,
                int(args.fp16 and device.type == "cuda"),
                int(args.bilateral),
                frame.shape[1],
                frame.shape[0],
                f"{fps:.3f}",
            ])
            csv_file.flush()

            idx += 1
            if args.max_frames and idx >= args.max_frames:
                break

            key = cv.waitKey(1) & 0xFF
            # Permitir salir con 'q', 'x' o ESC (27)
            if key in (ord('q'), ord('x'), 27):
                break
            if key == ord('s'):
                base = f"snap_{int(time.time())}"
                save_artifacts(depth, depth_u8, depth_color, out_dir, base, args)

    finally:
        cap.release()
        if writer_video is not None:
            writer_video.release()
            print(f"[INFO] Video guardado en {out_path_webcam}")
        cv.destroyAllWindows()
        csv_file.close()

"""##6. Resultados
Con la finalidad de validar la calidad de estimación del modelo seleccionado (MiDas), se ocuparon varias métricas típicas para evaluar estimaciones de profundidad.

*  **AbsRel** (Error Relativo Absoluto),  El error relativo medio entre la predicción y el valor real  normalizado por el valor real.
*  **RMSE** (Raíz del Error cuadrático medio) midiendo desviación típica del error en unidades absolutas (ej. metros)
* **δ a umbrales** (Precisión de Umbral)  que mide la proporción de predicciones “cercanas” al valor real dentro de un factor multiplicativo.

Se utilizó el conjunto de datos KITTI, obtenido desde un vehículo real equipado con cámaras, un sensor LIDAR, GPS e IMU, recorriendo calles y carreteras. Contiene datos sincronizados en video, imágenes estéreo, nubes de puntos y medidas de posición, que permite abordar tareas como estimación de profundidad, detección de objetos y reconstrucción 3D.

Las imágenes del dataset ya incluyen codificados los valores de profundidad obtenidos a partir de las mediciones reales del LIDAR. Estos valores están almacenados como enteros (generalmente en milímetros) y deben decodificarse para usarlos en metros, aunque solo están presentes en los píxeles donde el sensor pudo medir [4].

---
[4] Vision meets Robotics: The KITTI Dataset, Andreas Geiger, Philip Lenz, Christoph Stiller and Raquel Urtasun.

Para establecer los resultados se procesaron con MiDaS 16 imágenes tomadas del dataset Kitti y se calcularon las métricas anteriormente mencionadas, obteniendo los siguientes resultados:  


*   **AbsRel 0.57 / δ<1.25 = 0.21**: el error relativo es alto y solo ~21% de los píxeles cae dentro de ±25% del GT.
Esto es esperable con modelos no métricos (MiDaS) evaluados con una única reescala (median/ls): queda un sesgo aditivo no corregido.
*  **RMSE 14.53 m**: valores de metros grandes reflejan tanto el offset como outliers a larga distancia.

![metricas.PNG](data:image/png;base64,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)

## 7. Conclusiones

El proyecto logró desarrollar un sistema funcional de estimación de profundidad monocular en tiempo real, cumpliendo con el objetivo de integrar técnicas tradicionales de procesamiento digital de imágenes con enfoques de *deep learning*. A través de un estudio inicial de los métodos más utilizados para la detección de profundidad en visión por computador, se identificaron diferentes modelos de repositorios públicos, de los que fueron probados MiDaS, Patch Fusion y Marigold, seleccionando finalmente MiDaS (Intel-ISL) por su disponibilidad en PyTorch y su facilidad de adaptación a entornos de captura y hardware diversos. El sistema implementado se diseñó como una aplicación parametrizable por CLI, capaz de procesar tanto video en tiempo real como lotes de imágenes, lo que demuestra una correcta adaptación y optimización del código base utilizado.

La validación con el dataset KITTI permitió cuantificar el rendimiento del modelo, evidenciando métricas de error relativamente altas. Este resultado sugiere que el preacondicionamiento aplicado a las imágenes captadas (incluyendo normalización fotométrica, realce de bordes, máscaras morfológicas, redimensionado) pudo introducir incertidumbres que afectaron negativamente la precisión. No obstante, este hallazgo es valioso, ya que evidencia la sensibilidad de los modelos de *deep learning* a las transformaciones previas y abre líneas claras de mejora, como la optimización del flujo de preprocesamiento o el ajuste fino del modelo en datos más cercanos a las condiciones reales de captura. El trabajo demuestra la viabilidad de combinar procesamiento clásico y aprendizaje profundo para resolver problemas concretos de visión por computador, así como la importancia de una validación cuidadosa para orientar futuras optimizaciones.


Como recomendación para trabajos futuros, se sugiere realizar entrenamientos y validaciones utilizando otros modelos disponibles en la familia de MiDaS, como DPT_Large y MiDaS_small, dado que en este proyecto se trabajó exclusivamente con DPT_Hybrid. Esta comparación permitiría evaluar si arquitecturas con mayor capacidad o menor complejidad pueden ofrecer un mejor equilibrio entre precisión y velocidad, optimizando así el rendimiento del sistema desarrollado.

## 8. Referencias

1. https://medium.com/@patriciogv/the-state-of-the-art-of-depth-estimation-from-single-images-9e245d51a315
2. https://viso.ai/computer-vision/monocular-depth-estimation/
3. https://www.ipol.im/pub/art/2023/459/?utm_source=doi
4. https://arxiv.org/abs/2501.11841
5. https://arxiv.org/pdf/2104.06456
6. https://github.com/superKabe/Depth-Estimation-on-KITTI
"""